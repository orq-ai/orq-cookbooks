{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx_H1b1ldLHp"
      },
      "source": [
        "## **Capturing and Leveraging Feedback in Chatbots**\n",
        "This cookbook covers how to seamlessly log and manage user feedback in your FAQ chatbot using Orq.ai. Whether you're tracking quality issues, user actions, or overall sentiment, structured feedback logging helps you continuously improve responses and enhance user experience.\n",
        "\n",
        "Instead of relying on vague insights or manual reviews, Orq.ai‚Äôs feedback logging system allows you to:\n",
        "\n",
        "‚úÖ Capture **real-time user ratings** (good/bad) on chatbot responses\n",
        "\n",
        "‚úÖ **Log specific defects** like grammatical errors, hallucinations, or ambiguity\n",
        "\n",
        "‚úÖ Maintain structured conversation history to **refine future responses**\n",
        "\n",
        "By integrating feedback logging, you create a chatbot that learns from user input and evolves over time‚Äîno guesswork, just data-driven improvements!\n",
        "\n",
        "####**Step 1: Install Dependencies**\n",
        "\n",
        "Before starting, ensure you have an Orq account. If not, sign up first. Replace the API key, and you‚Äôre ready to go. For more advanced topics, check out the Orq documentation.\n",
        "\n",
        "Start by installing the required packages to use the Orq SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4h3VZ3tYtOs",
        "outputId": "d6e4f66b-a89a-4b6f-9286-86ce2c2f724d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting orq-ai-sdk\n",
            "  Downloading orq_ai_sdk-3.3.9-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting eval-type-backport>=0.2.0 (from orq-ai-sdk)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from orq-ai-sdk) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from orq-ai-sdk) (2.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from orq-ai-sdk) (2.8.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from orq-ai-sdk) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->orq-ai-sdk) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->orq-ai-sdk) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->orq-ai-sdk) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->orq-ai-sdk) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->orq-ai-sdk) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->orq-ai-sdk) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->orq-ai-sdk) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->orq-ai-sdk) (4.13.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->orq-ai-sdk) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->orq-ai-sdk) (1.3.1)\n",
            "Downloading orq_ai_sdk-3.3.9-py3-none-any.whl (185 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m185.7/185.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Installing collected packages: eval-type-backport, orq-ai-sdk\n",
            "Successfully installed eval-type-backport-0.2.2 orq-ai-sdk-3.3.9\n"
          ]
        }
      ],
      "source": [
        "pip install orq-ai-sdk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORhqf0zxdv_u"
      },
      "source": [
        "#### **Step 2: Set Up the Orq Client**\n",
        "\n",
        "Next, set up the Orq client using your API key. Replace the placeholder with your actual API key."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from orq_ai_sdk import Orq\n",
        "\n",
        "# Initialize Orq (Standalone Block for Initialization)\n",
        "api_key = os.getenv(\"ORQ_API_KEY\", \"your_api_key_here\")\n",
        "client = Orq(api_key=api_key)  # Ensure client is available for chat_with_deployment\n",
        "orq = client  # Maintain consistency for feedback logging"
      ],
      "metadata": {
        "id": "hHjbb9DLL2rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc5izNH-d5Lv"
      },
      "source": [
        "#### **Step 3: Setting Up a Knowledge Base in Orq.ai**\n",
        "\n",
        "To power the FAQ bot, you'll need a knowledge base containing relevant documents. In Orq.ai, knowledge bases are built using vector embeddings, enabling the bot to retrieve the most relevant information for any query.\n",
        "\n",
        "For this setup, we scraped our technical documentation and uploaded it to the knowledge base via the Orq platform. Keep in mind that this approach does not ensure continuous updates ‚Äî any changes to your documentation will need to be manually re-uploaded.\n",
        "\n",
        "To upload a knowledge base in Orq.ai:\n",
        "\n",
        "1. Create a New Knowledge Base in the Orq workspace.\n",
        "2. Upload Documents by dragging files.\n",
        "3. Process the Files to generate vector embeddings, making your content searchable by the bot.\n",
        "\n",
        "For a more detailed explanation, see the documentation.\n",
        "\n",
        "#### **Step 4: Orq FAQ Chat Prompt**\n",
        "\n",
        "This prompt defines the behavior of Orq.ai‚Äôs FAQ bot, ensuring responses are accurate, context-driven, and based only on the provided knowledge base. The assistant acts as a customer service agent, delivering factual answers while avoiding speculation.\n",
        "\n",
        "The prompt includes clear instructions to maintain professionalism:\n",
        "\n",
        "‚úÖ Use only the knowledge base for answers\n",
        "\n",
        "‚úÖ Express uncertainty when information is unclear\n",
        "\n",
        "‚úÖ Avoid opinions or assumptions\n",
        "\n",
        "‚úÖ Break down complex topics into simple explanations\n",
        "\n",
        "‚úÖ Use objective, neutral language\n",
        "\n",
        "This ensures reliable and well-supported answers for users across various contexts.\n",
        "\n",
        "This is the general prompt in Orq.ai:\n",
        "\n",
        "```plaintext\n",
        "#### Role\n",
        "You are a customer service assistant working for Orq.ai specialized in answering questions as accurately and factually as possible given all provided context. If there is no provided context, don‚Äôt answer the question but say: ‚Äúsorry I don‚Äôt have information to answer your question‚Äù. Your goal is to provide clear, concise, and well-supported answers based on information from a knowledge base.\n",
        "\n",
        "### Instructions\n",
        "When responding:\n",
        "* Express uncertainty on unclear or debatable topics\n",
        "* Avoid speculation or personal opinions\n",
        "* Break down complex topics into understandable explanations\n",
        "* Use objective, neutral language\n",
        "\n",
        "When asked a question:\n",
        "ONLY use the following data coming from a knowledge base to answer your question:\n",
        "\n",
        "<data_you_can_use>\n",
        "{{orq_technical_docs}}\n",
        "</data_you_can_use>```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdwc47UpenmS"
      },
      "source": [
        "#### **Step 6: Define the Interaction Function**\n",
        "\n",
        "The bot requires a function to process user input, maintain conversation history, and interact with the RAG deployment. Additionally, we extract and store the trace ID, which is essential for logging user feedback. Below is a sample implementation:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_deployment(message, conv_memory, language):\n",
        "    conv_memory.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    generation = client.deployments.invoke(\n",
        "        key=\"orqai_FAQ_bot_RAGAS\",\n",
        "        context={\n",
        "            \"environments\": [\"production\"],\n",
        "        },\n",
        "        metadata={\"custom-field-name\": \"custom-metadata-value\"},\n",
        "        messages=conv_memory\n",
        "    )\n",
        "\n",
        "    # Store trace id as this is needed for logging feedback\n",
        "    id =  generation.id\n",
        "\n",
        "    # Extract response content\n",
        "    response = generation.choices[0].message.content\n",
        "\n",
        "    # Store both full_id and extracted id in conversation memory\n",
        "    conv_memory.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "    return response, id  # Return both full_id and extracted id"
      ],
      "metadata": {
        "id": "oEgo-xoe2GI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0DBfLHenJ3v"
      },
      "source": [
        "#### **Step 8: Run Your FAQ Bot**\n",
        "\n",
        "In a real deployment, feedback would be collected through front-end buttons (e.g., thumbs-up/down, dropdowns, or action buttons). For demonstration purposes, we simulate this process in the notebook using text-based inputs.\n",
        "\n",
        "####**How the Feedback Loop Works:**\n",
        "\n",
        "1. **User Rating** ‚Äì After each response, users mark it as good or bad to signal quality.\n",
        "\n",
        "2. **Logging Context** ‚Äì If bad, the bot stores:\n",
        "‚ÄúREMEMBER '[response]' was a bad response to '[question]'‚Äù\n",
        "This helps the model learn from past mistakes.\n",
        "\n",
        "3. **Defect Classification** ‚Äì Users specify the issue (e.g., grammatical, hallucination, off-topic) for targeted improvements.\n",
        "\n",
        "By structuring feedback, we create a continuous learning loop, improving chatbot reliability, adaptability, and user experience‚Äîwithout relying on guesswork. üöÄ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "defect_options = [\n",
        "    \"grammatical\", \"spelling\", \"hallucination\", \"repetition\", \"inappropriate\", \"off_topic\", \"incompleteness\", \"ambiguity\"\n",
        "]\n",
        "\n",
        "def chatbot():\n",
        "    conv_memory = []\n",
        "    trace_ids = []\n",
        "\n",
        "    print(\"\\nYou can now start chatting! Type 'exit' or 'quit' to end the chat.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Ending chat.\")\n",
        "            break\n",
        "\n",
        "        # Get model response\n",
        "        response, trace_id = chat_with_deployment(user_input, conv_memory, client)\n",
        "        trace_ids.append(trace_id)\n",
        "        print(f\"Assistant (Extracted ID: {trace_id}): {response}\")\n",
        "\n",
        "        # Get feedback\n",
        "        feedback = input(\"Provide feedback (good/bad) or press Enter to skip: \").strip().lower()\n",
        "        if feedback in [\"good\", \"bad\"]:\n",
        "            res = orq.feedback.create(field=\"rating\", value=[feedback], trace_id=trace_id)\n",
        "            print(f\"Feedback logged: {res}\")\n",
        "\n",
        "            # If feedback is bad, append a structured message to conversation history and request defect details\n",
        "            if feedback == \"bad\":\n",
        "                conv_memory.append({\"role\": \"user\", \"content\": f\"REMEMBER '{response}' was a bad response to the following question: '{user_input}'\"})\n",
        "\n",
        "                # Request defect type\n",
        "                defect_feedback = input(\"What was wrong with the response? (Choose from: grammatical, spelling, hallucination, repetition, inappropriate, off_topic, incompleteness, ambiguity): \").strip().lower()\n",
        "\n",
        "                if defect_feedback in defect_options:\n",
        "                    defect_res = orq.feedback.create(field=\"defects\", value=[defect_feedback], trace_id=trace_id)\n",
        "                    print(f\"Defect feedback logged: {defect_res}\")\n",
        "                else:\n",
        "                    print(\"Invalid defect type. No defect feedback logged.\")\n",
        "\n",
        "# Run chatbot\n",
        "chatbot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwKngFRwMBPQ",
        "outputId": "aaf5b7e2-7a73-4267-f9b9-9bd15d721ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You can now start chatting! Type 'exit' or 'quit' to end the chat.\n",
            "\n",
            "You: what is openai?\n",
            "Assistant (Extracted ID: 01JPMKG5XFVGW6F1RHFA5B1AAD): OpenAI is an artificial intelligence research laboratory founded in 2015, known for developing advanced AI models like GPT (Generative Pre-trained Transformer) and DALL-E. They create powerful language and image generation models that can understand and generate human-like text and images. OpenAI's models are widely used for various applications such as chatbots, content creation, code generation, and creative tasks. Their most famous product is ChatGPT, a conversational AI that can engage in human-like dialogue and assist with a wide range of tasks.\n",
            "Provide feedback (good/bad) or press Enter to skip: bad\n",
            "Feedback logged: property='rating' value=['bad'] trace_id='01JPMKG5XFVGW6F1RHFA5B1AAD' id='01JPMKGD5J9E7DM2G79NJACYX0'\n",
            "What was wrong with the response? (Choose from: grammatical, spelling, hallucination, repetition, inappropriate, off_topic, incompleteness, ambiguity): incompleteness\n",
            "Defect feedback logged: property='defects' value=['incompleteness'] trace_id='01JPMKG5XFVGW6F1RHFA5B1AAD' id='01JPMKGHJXRNCWPZQ62KYNR608'\n",
            "You: what is openai?\n",
            "Assistant (Extracted ID: 01JPMKGS6WENJT6RHB40G252Y7): OpenAI is an AI research company that provides AI models and APIs for developers to integrate advanced language and image generation capabilities into their applications. Through their platform, developers can access models like GPT for text generation and DALL-E for image creation, enabling various AI-powered functionalities in software products.\n",
            "Provide feedback (good/bad) or press Enter to skip: good\n",
            "Feedback logged: property='rating' value=['good'] trace_id='01JPMKGS6WENJT6RHB40G252Y7' id='01JPMKHEPH9HKF7T60ZMGEX78H'\n",
            "You: exit\n",
            "Ending chat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIe1bGOIm4fC"
      },
      "source": [
        "#### **Next Steps**\n",
        "Great job! You‚Äôve implemented a structured feedback loop for your FAQ bot, ensuring continuous learning and response improvement. To take it further:\n",
        "\n",
        "* **Integrate interaction tracking** ‚Äì Link front-end actions (**copied, saved, deleted, shared)** to feedback logging, allowing the bot to learn without requiring explicit user input.\n",
        "\n",
        "* **Create annotated datasets in Orq** ‚Äì Use feedback as a selection method to build **curated datasets** for evaluation. Run experiments to see if updates to prompts, models, parameters, or the knowledge base improve performance and response quality.\n",
        "\n",
        "By embedding feedback directly into user interactions, you create a **frictionless improvement cycle**, making your FAQ bot more adaptive and user-friendly.\n",
        "\n",
        "For more resources and advanced features, visit the Orq.ai Documentation."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JM_3vIAZLL_6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}