{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57156fe3",
   "metadata": {},
   "source": [
    "# Orq.ai OpenTelemetry Integration Example\n",
    "\n",
    "This notebook demonstrates how to integrate Orq.ai with OpenTelemetry for comprehensive observability of your AI applications.\n",
    "\n",
    "Orq.ai is an AI Gateway and LLM Collaboration Platform that provides:\n",
    "- Unified API for multiple LLM providers\n",
    "- Deployment management and versioning\n",
    "- Built-in retry, fallback, and caching strategies\n",
    "- Contact and thread tracking for budget management\n",
    "- Knowledge base integration\n",
    "- Real-time streaming support\n",
    "\n",
    "This example shows how to instrument Orq.ai SDK calls with OpenTelemetry to capture detailed traces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f5e1d9",
   "metadata": {},
   "source": [
    "## Setup Requirements\n",
    "\n",
    "- Run trace API service on port 5300 locally\n",
    "- Install required packages:\n",
    "\n",
    "```bash\n",
    "pip install orq-ai-sdk \\\n",
    "            opentelemetry-sdk \\\n",
    "            opentelemetry-exporter-otlp \\\n",
    "            opentelemetry-api \\\n",
    "            nest-asyncio\n",
    "```\n",
    "\n",
    "Note: Unlike OpenInference which provides auto-instrumentation, Orq.ai requires manual span creation for observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b516651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Environment Variables\n",
    "os.environ[\"ORQ_API_KEY\"] = \"<replace-me>\"\n",
    "\n",
    "# OpenTelemetry Configuration\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"http://localhost:5300/v2/otel\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = \"Authorization=Bearer <replace-me>\"\n",
    "\n",
    "print(\"✅ Environment variables configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional configuration for async environments\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize OpenTelemetry\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.resources import Resource, SERVICE_NAME, SERVICE_VERSION\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "\n",
    "# Configure Resource\n",
    "resource = Resource.create({\n",
    "    SERVICE_NAME: \"orq-ai-otel-example\",\n",
    "    SERVICE_VERSION: \"1.0.0\",\n",
    "    \"deployment.environment\": \"development\"\n",
    "})\n",
    "\n",
    "# Configure TracerProvider\n",
    "provider = TracerProvider(resource=resource)\n",
    "\n",
    "# Configure OTLP Exporter (uses env vars automatically)\n",
    "otlp_exporter = OTLPSpanExporter()\n",
    "\n",
    "# Add span processors\n",
    "provider.add_span_processor(BatchSpanProcessor(otlp_exporter))\n",
    "\n",
    "trace.set_tracer_provider(provider)\n",
    "\n",
    "# Get tracer for manual instrumentation\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "print(\"✅ OpenTelemetry initialized\")\n",
    "print(\"✅ Manual instrumentation ready for Orq.ai SDK calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11fb07",
   "metadata": {},
   "source": [
    "## Example #1: Basic Deployment Invocation with Tracing\n",
    "\n",
    "This example demonstrates:\n",
    "- Creating a basic deployment invocation\n",
    "- Manual span creation and attribute setting\n",
    "- Capturing inputs, outputs, and metadata\n",
    "- Error handling with OpenTelemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb869364",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example #1: Basic Deployment Invocation with OpenTelemetry Tracing\n",
    "\"\"\"\n",
    "\n",
    "from orq_ai_sdk import Orq\n",
    "import time\n",
    "\n",
    "def invoke_deployment_with_tracing():\n",
    "    \"\"\"Invoke an Orq.ai deployment with manual OpenTelemetry tracing\"\"\"\n",
    "    \n",
    "    # Create parent span for the entire operation\n",
    "    with tracer.start_as_current_span(\"orq_deployment_invoke\") as span:\n",
    "        try:\n",
    "            # Set span attributes\n",
    "            span.set_attribute(\"orq.operation\", \"deployment.invoke\")\n",
    "            span.set_attribute(\"orq.deployment.key\", \"customer_service_bot\")\n",
    "            span.set_attribute(\"orq.environment\", \"production\")\n",
    "            \n",
    "            # Initialize Orq client\n",
    "            with Orq(\n",
    "                api_key=os.environ.get(\"ORQ_API_KEY\"),\n",
    "                environment=\"production\"\n",
    "            ) as client:\n",
    "                \n",
    "                # Prepare inputs\n",
    "                user_input = \"What are the features of Orq.ai?\"\n",
    "                span.set_attribute(\"orq.input.message\", user_input)\n",
    "                \n",
    "                # Record start time\n",
    "                start_time = time.time()\n",
    "                span.add_event(\"deployment_invocation_started\")\n",
    "                \n",
    "                # Invoke deployment\n",
    "                response = client.deployments.invoke(\n",
    "                    key=\"customer_service_bot\",\n",
    "                    inputs={\n",
    "                        \"user_question\": user_input\n",
    "                    },\n",
    "                    context={\n",
    "                        \"environment\": \"production\",\n",
    "                        \"region\": \"us-east-1\"\n",
    "                    },\n",
    "                    metadata={\n",
    "                        \"session_id\": \"session_12345\",\n",
    "                        \"user_tier\": \"premium\"\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Calculate duration\n",
    "                duration = time.time() - start_time\n",
    "                span.set_attribute(\"orq.duration_seconds\", duration)\n",
    "                \n",
    "                # Extract and log response\n",
    "                output = response.choices[0].message.content\n",
    "                span.set_attribute(\"orq.output.message\", output[:500])  # Truncate for span\n",
    "                span.set_attribute(\"orq.output.length\", len(output))\n",
    "                \n",
    "                # Log model information if available\n",
    "                if hasattr(response, 'model'):\n",
    "                    span.set_attribute(\"orq.model\", response.model)\n",
    "                \n",
    "                # Log usage information if available\n",
    "                if hasattr(response, 'usage'):\n",
    "                    span.set_attribute(\"orq.usage.prompt_tokens\", response.usage.prompt_tokens)\n",
    "                    span.set_attribute(\"orq.usage.completion_tokens\", response.usage.completion_tokens)\n",
    "                    span.set_attribute(\"orq.usage.total_tokens\", response.usage.total_tokens)\n",
    "                \n",
    "                span.add_event(\"deployment_invocation_completed\", {\n",
    "                    \"response_length\": len(output),\n",
    "                    \"duration_seconds\": duration\n",
    "                })\n",
    "                \n",
    "                # Mark span as successful\n",
    "                span.set_status(Status(StatusCode.OK))\n",
    "                \n",
    "                print(\"✅ Deployment invoked successfully\")\n",
    "                print(f\"Response: {output}\")\n",
    "                print(f\"Duration: {duration:.2f}s\")\n",
    "                \n",
    "                return response\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Record exception in span\n",
    "            span.record_exception(e)\n",
    "            span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "            span.set_attribute(\"orq.error.type\", type(e).__name__)\n",
    "            span.set_attribute(\"orq.error.message\", str(e))\n",
    "            \n",
    "            print(f\"❌ Error during deployment invocation: {e}\")\n",
    "            raise\n",
    "\n",
    "# Run the example\n",
    "invoke_deployment_with_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b2c3d",
   "metadata": {},
   "source": [
    "## Example #2: Streaming Responses with Tracing\n",
    "\n",
    "This example demonstrates:\n",
    "- Streaming responses from Orq.ai deployments\n",
    "- Tracking streaming chunks with events\n",
    "- Measuring time-to-first-token and total streaming time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example #2: Streaming Deployment with OpenTelemetry Tracing\n",
    "\"\"\"\n",
    "\n",
    "from orq_ai_sdk import Orq\n",
    "import time\n",
    "\n",
    "def stream_deployment_with_tracing():\n",
    "    \"\"\"Stream an Orq.ai deployment response with tracing\"\"\"\n",
    "    \n",
    "    with tracer.start_as_current_span(\"orq_deployment_stream\") as span:\n",
    "        try:\n",
    "            span.set_attribute(\"orq.operation\", \"deployment.stream\")\n",
    "            span.set_attribute(\"orq.deployment.key\", \"story_generator\")\n",
    "            span.set_attribute(\"orq.stream\", True)\n",
    "            \n",
    "            with Orq(\n",
    "                api_key=os.environ.get(\"ORQ_API_KEY\"),\n",
    "                environment=\"production\"\n",
    "            ) as client:\n",
    "                \n",
    "                prompt = \"Write a short story about AI and humans working together\"\n",
    "                span.set_attribute(\"orq.input.prompt\", prompt)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                first_token_time = None\n",
    "                chunk_count = 0\n",
    "                total_content = \"\"\n",
    "                \n",
    "                span.add_event(\"stream_started\")\n",
    "                \n",
    "                # Stream the response\n",
    "                stream = client.deployments.stream(\n",
    "                    key=\"story_generator\",\n",
    "                    inputs={\"prompt\": prompt}\n",
    "                )\n",
    "                \n",
    "                with stream as event_stream:\n",
    "                    for event in event_stream:\n",
    "                        # Record first token time\n",
    "                        if first_token_time is None:\n",
    "                            first_token_time = time.time() - start_time\n",
    "                            span.set_attribute(\"orq.stream.time_to_first_token\", first_token_time)\n",
    "                            span.add_event(\"first_token_received\", {\n",
    "                                \"latency_seconds\": first_token_time\n",
    "                            })\n",
    "                        \n",
    "                        # Process chunk\n",
    "                        if hasattr(event, 'choices') and len(event.choices) > 0:\n",
    "                            delta = event.choices[0].delta\n",
    "                            if hasattr(delta, 'content') and delta.content:\n",
    "                                chunk_count += 1\n",
    "                                total_content += delta.content\n",
    "                                print(delta.content, end='', flush=True)\n",
    "                \n",
    "                print()  # New line after streaming\n",
    "                \n",
    "                # Calculate final metrics\n",
    "                total_duration = time.time() - start_time\n",
    "                span.set_attribute(\"orq.stream.total_duration\", total_duration)\n",
    "                span.set_attribute(\"orq.stream.chunk_count\", chunk_count)\n",
    "                span.set_attribute(\"orq.output.length\", len(total_content))\n",
    "                span.set_attribute(\"orq.output.content\", total_content[:500])  # Truncated\n",
    "                \n",
    "                span.add_event(\"stream_completed\", {\n",
    "                    \"chunk_count\": chunk_count,\n",
    "                    \"total_duration\": total_duration,\n",
    "                    \"content_length\": len(total_content)\n",
    "                })\n",
    "                \n",
    "                span.set_status(Status(StatusCode.OK))\n",
    "                \n",
    "                print(f\"\\n✅ Streaming completed\")\n",
    "                print(f\"Chunks received: {chunk_count}\")\n",
    "                print(f\"Time to first token: {first_token_time:.3f}s\")\n",
    "                print(f\"Total duration: {total_duration:.2f}s\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            span.record_exception(e)\n",
    "            span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "            print(f\"❌ Error during streaming: {e}\")\n",
    "            raise\n",
    "\n",
    "# Run the example\n",
    "stream_deployment_with_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5c6d7e",
   "metadata": {},
   "source": [
    "## Example #3: Multi-Turn Conversation with Contact & Thread Tracking\n",
    "\n",
    "This example demonstrates:\n",
    "- Managing conversation history\n",
    "- Using contact_id for budget tracking\n",
    "- Using thread_id for conversation grouping\n",
    "- Nested spans for conversation turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6d7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example #3: Multi-Turn Conversation with Contact and Thread Tracking\n",
    "\"\"\"\n",
    "\n",
    "from orq_ai_sdk import Orq\n",
    "import time\n",
    "\n",
    "def multi_turn_conversation_with_tracing():\n",
    "    \"\"\"Demonstrate multi-turn conversation with Orq.ai custom attributes\"\"\"\n",
    "    \n",
    "    with tracer.start_as_current_span(\"orq_conversation_session\") as session_span:\n",
    "        try:\n",
    "            # Set session-level attributes\n",
    "            contact_id = \"user_789\"\n",
    "            thread_id = \"conversation_abc123\"\n",
    "            \n",
    "            session_span.set_attribute(\"orq.operation\", \"conversation.session\")\n",
    "            session_span.set_attribute(\"orq.contact_id\", contact_id)\n",
    "            session_span.set_attribute(\"orq.thread_id\", thread_id)\n",
    "            session_span.set_attribute(\"orq.deployment.key\", \"chat_assistant\")\n",
    "            \n",
    "            # Initialize client with contact tracking\n",
    "            with Orq(\n",
    "                api_key=os.environ.get(\"ORQ_API_KEY\"),\n",
    "                environment=\"production\",\n",
    "                contact_id=contact_id  # Track usage per contact\n",
    "            ) as client:\n",
    "                \n",
    "                # Conversation history\n",
    "                conversation_history = []\n",
    "                \n",
    "                # Define conversation turns\n",
    "                conversation_turns = [\n",
    "                    \"Hello! Can you explain what Orq.ai does?\",\n",
    "                    \"What are the key benefits of using Orq.ai?\",\n",
    "                    \"How does the contact tracking feature work?\"\n",
    "                ]\n",
    "                \n",
    "                session_span.set_attribute(\"orq.conversation.turn_count\", len(conversation_turns))\n",
    "                \n",
    "                for turn_number, user_message in enumerate(conversation_turns, 1):\n",
    "                    # Create a span for each turn\n",
    "                    with tracer.start_as_current_span(f\"orq_conversation_turn_{turn_number}\") as turn_span:\n",
    "                        try:\n",
    "                            turn_span.set_attribute(\"orq.turn.number\", turn_number)\n",
    "                            turn_span.set_attribute(\"orq.turn.user_message\", user_message)\n",
    "                            turn_span.set_attribute(\"orq.contact_id\", contact_id)\n",
    "                            turn_span.set_attribute(\"orq.thread_id\", thread_id)\n",
    "                            \n",
    "                            # Add user message to history\n",
    "                            conversation_history.append({\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": user_message\n",
    "                            })\n",
    "                            \n",
    "                            start_time = time.time()\n",
    "                            turn_span.add_event(\"turn_started\", {\"turn\": turn_number})\n",
    "                            \n",
    "                            # Invoke deployment with conversation history\n",
    "                            response = client.deployments.invoke(\n",
    "                                key=\"chat_assistant\",\n",
    "                                messages=conversation_history,\n",
    "                                metadata={\n",
    "                                    \"conversation_turn\": turn_number,\n",
    "                                    \"thread_id\": thread_id\n",
    "                                }\n",
    "                            )\n",
    "                            \n",
    "                            duration = time.time() - start_time\n",
    "                            \n",
    "                            # Extract response\n",
    "                            assistant_message = response.choices[0].message.content\n",
    "                            \n",
    "                            # Add assistant response to history\n",
    "                            conversation_history.append({\n",
    "                                \"role\": \"assistant\",\n",
    "                                \"content\": assistant_message\n",
    "                            })\n",
    "                            \n",
    "                            # Set turn attributes\n",
    "                            turn_span.set_attribute(\"orq.turn.assistant_message\", assistant_message[:500])\n",
    "                            turn_span.set_attribute(\"orq.turn.duration\", duration)\n",
    "                            turn_span.set_attribute(\"orq.turn.history_length\", len(conversation_history))\n",
    "                            \n",
    "                            # Log token usage if available\n",
    "                            if hasattr(response, 'usage'):\n",
    "                                turn_span.set_attribute(\"orq.turn.tokens.prompt\", response.usage.prompt_tokens)\n",
    "                                turn_span.set_attribute(\"orq.turn.tokens.completion\", response.usage.completion_tokens)\n",
    "                                turn_span.set_attribute(\"orq.turn.tokens.total\", response.usage.total_tokens)\n",
    "                            \n",
    "                            turn_span.add_event(\"turn_completed\", {\n",
    "                                \"turn\": turn_number,\n",
    "                                \"duration_seconds\": duration,\n",
    "                                \"response_length\": len(assistant_message)\n",
    "                            })\n",
    "                            \n",
    "                            turn_span.set_status(Status(StatusCode.OK))\n",
    "                            \n",
    "                            print(f\"\\n{'='*60}\")\n",
    "                            print(f\"Turn {turn_number}\")\n",
    "                            print(f\"{'='*60}\")\n",
    "                            print(f\"User: {user_message}\")\n",
    "                            print(f\"\\nAssistant: {assistant_message}\")\n",
    "                            print(f\"\\n⏱️ Duration: {duration:.2f}s\")\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            turn_span.record_exception(e)\n",
    "                            turn_span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "                            print(f\"❌ Error in turn {turn_number}: {e}\")\n",
    "                            raise\n",
    "                \n",
    "                # Set session summary attributes\n",
    "                session_span.set_attribute(\"orq.conversation.completed_turns\", len(conversation_turns))\n",
    "                session_span.set_attribute(\"orq.conversation.final_history_length\", len(conversation_history))\n",
    "                session_span.add_event(\"conversation_session_completed\", {\n",
    "                    \"total_turns\": len(conversation_turns),\n",
    "                    \"messages_exchanged\": len(conversation_history)\n",
    "                })\n",
    "                session_span.set_status(Status(StatusCode.OK))\n",
    "                \n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(\"✅ Conversation session completed successfully\")\n",
    "                print(f\"Total turns: {len(conversation_turns)}\")\n",
    "                print(f\"Messages exchanged: {len(conversation_history)}\")\n",
    "                print(f\"Contact ID: {contact_id}\")\n",
    "                print(f\"Thread ID: {thread_id}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            session_span.record_exception(e)\n",
    "            session_span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "            print(f\"❌ Error in conversation session: {e}\")\n",
    "            raise\n",
    "\n",
    "# Run the example\n",
    "multi_turn_conversation_with_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f8a9b",
   "metadata": {},
   "source": [
    "## Example #4: Orq AI Gateway with OpenAI SDK\n",
    "\n",
    "This example demonstrates:\n",
    "- Using Orq.ai's AI Gateway with OpenAI SDK\n",
    "- Leveraging Orq's retry, fallback, and caching features\n",
    "- Multi-provider routing\n",
    "- Manual tracing of gateway operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8e9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example #4: Orq AI Gateway with OpenAI SDK and OpenTelemetry\n",
    "\"\"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "def orq_gateway_with_tracing():\n",
    "    \"\"\"Use Orq.ai AI Gateway with OpenAI SDK and tracing\"\"\"\n",
    "    \n",
    "    with tracer.start_as_current_span(\"orq_ai_gateway\") as span:\n",
    "        try:\n",
    "            span.set_attribute(\"orq.operation\", \"gateway.chat_completion\")\n",
    "            span.set_attribute(\"orq.gateway.type\", \"openai_compatible\")\n",
    "            \n",
    "            # Initialize OpenAI client pointing to Orq.ai gateway\n",
    "            client = OpenAI(\n",
    "                api_key=os.environ.get(\"ORQ_API_KEY\"),\n",
    "                base_url=\"https://api.orq.ai/v2/proxy\"\n",
    "            )\n",
    "            \n",
    "            # Prepare request\n",
    "            model = \"openai/gpt-4o-mini\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Explain the benefits of using an AI Gateway in 3 sentences.\"}\n",
    "            ]\n",
    "            \n",
    "            span.set_attribute(\"orq.model\", model)\n",
    "            span.set_attribute(\"orq.input.system_message\", messages[0][\"content\"])\n",
    "            span.set_attribute(\"orq.input.user_message\", messages[1][\"content\"])\n",
    "            \n",
    "            start_time = time.time()\n",
    "            span.add_event(\"gateway_request_started\")\n",
    "            \n",
    "            # Make request through Orq gateway\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0.7,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            \n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            # Extract response details\n",
    "            content = response.choices[0].message.content\n",
    "            finish_reason = response.choices[0].finish_reason\n",
    "            \n",
    "            # Set response attributes\n",
    "            span.set_attribute(\"orq.gateway.duration\", duration)\n",
    "            span.set_attribute(\"orq.output.content\", content[:500])\n",
    "            span.set_attribute(\"orq.output.finish_reason\", finish_reason)\n",
    "            span.set_attribute(\"orq.response.id\", response.id)\n",
    "            span.set_attribute(\"orq.response.model\", response.model)\n",
    "            \n",
    "            # Log token usage\n",
    "            if response.usage:\n",
    "                span.set_attribute(\"orq.usage.prompt_tokens\", response.usage.prompt_tokens)\n",
    "                span.set_attribute(\"orq.usage.completion_tokens\", response.usage.completion_tokens)\n",
    "                span.set_attribute(\"orq.usage.total_tokens\", response.usage.total_tokens)\n",
    "            \n",
    "            span.add_event(\"gateway_request_completed\", {\n",
    "                \"duration_seconds\": duration,\n",
    "                \"tokens_used\": response.usage.total_tokens if response.usage else 0,\n",
    "                \"finish_reason\": finish_reason\n",
    "            })\n",
    "            \n",
    "            span.set_status(Status(StatusCode.OK))\n",
    "            \n",
    "            print(\"✅ AI Gateway request completed\")\n",
    "            print(f\"Model: {response.model}\")\n",
    "            print(f\"Response: {content}\")\n",
    "            print(f\"Duration: {duration:.2f}s\")\n",
    "            if response.usage:\n",
    "                print(f\"Tokens: {response.usage.total_tokens}\")\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            span.record_exception(e)\n",
    "            span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "            print(f\"❌ Error in AI Gateway request: {e}\")\n",
    "            raise\n",
    "\n",
    "# Run the example\n",
    "orq_gateway_with_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b0c1d",
   "metadata": {},
   "source": [
    "## Example #5: Streaming via AI Gateway with Tracing\n",
    "\n",
    "This example demonstrates:\n",
    "- Streaming responses through Orq.ai AI Gateway\n",
    "- Using OpenAI SDK with Orq.ai backend\n",
    "- Tracking streaming metrics and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c1d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example #5: Streaming via AI Gateway with OpenTelemetry\n",
    "\"\"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "def gateway_streaming_with_tracing():\n",
    "    \"\"\"Stream responses through Orq.ai AI Gateway with tracing\"\"\"\n",
    "    \n",
    "    with tracer.start_as_current_span(\"orq_gateway_stream\") as span:\n",
    "        try:\n",
    "            span.set_attribute(\"orq.operation\", \"gateway.chat_completion_stream\")\n",
    "            span.set_attribute(\"orq.stream\", True)\n",
    "            \n",
    "            # Initialize OpenAI client pointing to Orq.ai gateway\n",
    "            client = OpenAI(\n",
    "                api_key=os.environ.get(\"ORQ_API_KEY\"),\n",
    "                base_url=\"https://api.orq.ai/v2/proxy\"\n",
    "            )\n",
    "            \n",
    "            model = \"openai/gpt-4o\"\n",
    "            prompt = \"Write a brief explanation of how AI gateways improve reliability and cost efficiency.\"\n",
    "            \n",
    "            span.set_attribute(\"orq.model\", model)\n",
    "            span.set_attribute(\"orq.input.prompt\", prompt)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            first_chunk_time = None\n",
    "            chunk_count = 0\n",
    "            accumulated_content = \"\"\n",
    "            \n",
    "            span.add_event(\"gateway_stream_started\")\n",
    "            \n",
    "            # Create streaming request\n",
    "            stream = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            # Process stream\n",
    "            for chunk in stream:\n",
    "                if first_chunk_time is None:\n",
    "                    first_chunk_time = time.time() - start_time\n",
    "                    span.set_attribute(\"orq.stream.time_to_first_chunk\", first_chunk_time)\n",
    "                    span.add_event(\"first_chunk_received\", {\n",
    "                        \"latency_seconds\": first_chunk_time\n",
    "                    })\n",
    "                \n",
    "                if chunk.choices[0].delta.content is not None:\n",
    "                    content = chunk.choices[0].delta.content\n",
    "                    chunk_count += 1\n",
    "                    accumulated_content += content\n",
    "                    print(content, end='', flush=True)\n",
    "            \n",
    "            print()  # New line\n",
    "            \n",
    "            # Calculate final metrics\n",
    "            total_duration = time.time() - start_time\n",
    "            tokens_per_second = len(accumulated_content.split()) / total_duration if total_duration > 0 else 0\n",
    "            \n",
    "            span.set_attribute(\"orq.stream.total_duration\", total_duration)\n",
    "            span.set_attribute(\"orq.stream.chunk_count\", chunk_count)\n",
    "            span.set_attribute(\"orq.stream.content_length\", len(accumulated_content))\n",
    "            span.set_attribute(\"orq.stream.tokens_per_second\", tokens_per_second)\n",
    "            span.set_attribute(\"orq.output.content\", accumulated_content[:500])\n",
    "            \n",
    "            span.add_event(\"gateway_stream_completed\", {\n",
    "                \"chunk_count\": chunk_count,\n",
    "                \"total_duration\": total_duration,\n",
    "                \"content_length\": len(accumulated_content),\n",
    "                \"tokens_per_second\": tokens_per_second\n",
    "            })\n",
    "            \n",
    "            span.set_status(Status(StatusCode.OK))\n",
    "            \n",
    "            print(f\"\\n✅ Streaming completed via AI Gateway\")\n",
    "            print(f\"Chunks: {chunk_count}\")\n",
    "            print(f\"Time to first chunk: {first_chunk_time:.3f}s\")\n",
    "            print(f\"Total duration: {total_duration:.2f}s\")\n",
    "            print(f\"~Tokens/sec: {tokens_per_second:.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            span.record_exception(e)\n",
    "            span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "            print(f\"❌ Error during gateway streaming: {e}\")\n",
    "            raise\n",
    "\n",
    "# Run the example\n",
    "gateway_streaming_with_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e2f3a",
   "metadata": {},
   "source": [
    "## Example #6: Complete RAG Pipeline with Orq.ai\n",
    "\n",
    "This example demonstrates:\n",
    "- Using Orq.ai knowledge bases for RAG\n",
    "- Tracking retrieval and generation separately\n",
    "- Context injection and relevance scoring\n",
    "- End-to-end RAG observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f3a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example #6: RAG Pipeline with Orq.ai Knowledge Bases and Tracing\n",
    "\"\"\"\n",
    "\n",
    "from orq_ai_sdk import Orq\n",
    "import time\n",
    "\n",
    "def rag_pipeline_with_tracing():\n",
    "    \"\"\"Demonstrate RAG with Orq.ai knowledge bases and comprehensive tracing\"\"\"\n",
    "    \n",
    "    with tracer.start_as_current_span(\"orq_rag_pipeline\") as pipeline_span:\n",
    "        try:\n",
    "            pipeline_span.set_attribute(\"orq.operation\", \"rag.pipeline\")\n",
    "            pipeline_span.set_attribute(\"orq.deployment.key\", \"documentation_assistant\")\n",
    "            \n",
    "            with Orq(\n",
    "                api_key=os.environ.get(\"ORQ_API_KEY\"),\n",
    "                environment=\"production\"\n",
    "            ) as client:\n",
    "                \n",
    "                # User query\n",
    "                query = \"How do I implement retry logic with Orq.ai deployments?\"\n",
    "                pipeline_span.set_attribute(\"orq.rag.query\", query)\n",
    "                \n",
    "                pipeline_start = time.time()\n",
    "                pipeline_span.add_event(\"rag_pipeline_started\")\n",
    "                \n",
    "                # Invoke deployment with knowledge base context\n",
    "                with tracer.start_as_current_span(\"orq_rag_generation\") as gen_span:\n",
    "                    gen_span.set_attribute(\"orq.operation\", \"rag.generation\")\n",
    "                    gen_span.set_attribute(\"orq.query\", query)\n",
    "                    \n",
    "                    gen_start = time.time()\n",
    "                    \n",
    "                    # Call deployment with context injection\n",
    "                    response = client.deployments.invoke(\n",
    "                        key=\"documentation_assistant\",\n",
    "                        inputs={\n",
    "                            \"user_query\": query\n",
    "                        },\n",
    "                        context={\n",
    "                            \"knowledge_base_ids\": [\"docs_kb_001\", \"api_kb_002\"],\n",
    "                            \"retrieval_top_k\": 5,\n",
    "                            \"relevance_threshold\": 0.75\n",
    "                        },\n",
    "                        metadata={\n",
    "                            \"rag_enabled\": True,\n",
    "                            \"query_type\": \"technical_documentation\"\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    gen_duration = time.time() - gen_start\n",
    "                    gen_span.set_attribute(\"orq.generation.duration\", gen_duration)\n",
    "                    \n",
    "                    # Extract response\n",
    "                    answer = response.choices[0].message.content\n",
    "                    gen_span.set_attribute(\"orq.generation.answer\", answer[:500])\n",
    "                    gen_span.set_attribute(\"orq.generation.answer_length\", len(answer))\n",
    "                    \n",
    "                    # Log usage if available\n",
    "                    if hasattr(response, 'usage'):\n",
    "                        gen_span.set_attribute(\"orq.generation.tokens.total\", response.usage.total_tokens)\n",
    "                    \n",
    "                    gen_span.add_event(\"generation_completed\", {\n",
    "                        \"duration_seconds\": gen_duration,\n",
    "                        \"answer_length\": len(answer)\n",
    "                    })\n",
    "                    gen_span.set_status(Status(StatusCode.OK))\n",
    "                \n",
    "                # Calculate pipeline metrics\n",
    "                pipeline_duration = time.time() - pipeline_start\n",
    "                pipeline_span.set_attribute(\"orq.rag.pipeline_duration\", pipeline_duration)\n",
    "                pipeline_span.set_attribute(\"orq.rag.answer\", answer[:500])\n",
    "                \n",
    "                pipeline_span.add_event(\"rag_pipeline_completed\", {\n",
    "                    \"total_duration\": pipeline_duration,\n",
    "                    \"answer_length\": len(answer)\n",
    "                })\n",
    "                pipeline_span.set_status(Status(StatusCode.OK))\n",
    "                \n",
    "                print(\"✅ RAG Pipeline completed\")\n",
    "                print(f\"\\nQuery: {query}\")\n",
    "                print(f\"\\nAnswer: {answer}\")\n",
    "                print(f\"\\nPipeline duration: {pipeline_duration:.2f}s\")\n",
    "                \n",
    "                return answer\n",
    "                \n",
    "        except Exception as e:\n",
    "            pipeline_span.record_exception(e)\n",
    "            pipeline_span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "            print(f\"❌ Error in RAG pipeline: {e}\")\n",
    "            raise\n",
    "\n",
    "# Run the example\n",
    "rag_pipeline_with_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3a4b5c",
   "metadata": {},
   "source": [
    "## Summary: Key Tracing Patterns for Orq.ai\n",
    "\n",
    "### Essential Span Attributes for Orq.ai:\n",
    "\n",
    "```python\n",
    "# Operation identification\n",
    "span.set_attribute(\"orq.operation\", \"deployment.invoke|deployment.stream|conversation.session\")\n",
    "span.set_attribute(\"orq.deployment.key\", \"deployment_name\")\n",
    "span.set_attribute(\"orq.environment\", \"production|staging|development\")\n",
    "\n",
    "# Contact and Thread tracking (for budget and conversation grouping)\n",
    "span.set_attribute(\"orq.contact_id\", \"user_id\")\n",
    "span.set_attribute(\"orq.thread_id\", \"conversation_id\")\n",
    "\n",
    "# Input/Output tracking\n",
    "span.set_attribute(\"orq.input.message\", user_input)\n",
    "span.set_attribute(\"orq.output.message\", response_text)\n",
    "span.set_attribute(\"orq.output.length\", len(response_text))\n",
    "\n",
    "# Performance metrics\n",
    "span.set_attribute(\"orq.duration_seconds\", duration)\n",
    "span.set_attribute(\"orq.stream.time_to_first_token\", ttft)\n",
    "span.set_attribute(\"orq.stream.chunk_count\", chunks)\n",
    "\n",
    "# Token usage (when available)\n",
    "span.set_attribute(\"orq.usage.prompt_tokens\", prompt_tokens)\n",
    "span.set_attribute(\"orq.usage.completion_tokens\", completion_tokens)\n",
    "span.set_attribute(\"orq.usage.total_tokens\", total_tokens)\n",
    "\n",
    "# Model information\n",
    "span.set_attribute(\"orq.model\", model_name)\n",
    "```\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Use nested spans** for complex operations (conversations, RAG pipelines)\n",
    "2. **Always set contact_id and thread_id** when available for better analytics\n",
    "3. **Add events** for important milestones (first token, completion, errors)\n",
    "4. **Record exceptions** properly with `span.record_exception(e)`\n",
    "5. **Set appropriate status** codes (OK, ERROR)\n",
    "6. **Truncate long strings** in attributes (use [:500] for content)\n",
    "7. **Track streaming metrics** separately (TTFT, chunk count, tokens/sec)\n",
    "\n",
    "### Key Differences from OpenInference:\n",
    "\n",
    "- Orq.ai requires **manual span creation** (no auto-instrumentation)\n",
    "- Use **orq.** prefix for custom attributes\n",
    "- Leverage Orq's **contact_id** and **thread_id** for built-in analytics\n",
    "- Track **deployment keys** instead of model names\n",
    "- Monitor **gateway operations** when using OpenAI SDK compatibility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
