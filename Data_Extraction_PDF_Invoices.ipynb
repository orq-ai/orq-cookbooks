{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lQ1rKpPyMIC"
      },
      "source": [
        "## **Data Extraction from PDF**\n",
        "\n",
        "Document extraction has always been a fascinating challenge. Over the years, advancements in AI have transformed this domain, making it easier to tackle even the most complex use cases. Using tools like Orq, extracting structured data from documents is now both efficient and practical. This cookbook demonstrates how to use Orq for processing PDF invoices, from file uploads to extracting actionable insights.\n",
        "\n",
        "To get started, you'll need to [sign up](https://orq.ai/create-account) for an Orq account if you haven't already.\n",
        "\n",
        "Additionally, we've prepared this [Google Colab](https://colab.research.google.com/drive/1QR1H2PTQhSB5ST29s-tHKCqfUnxU0R-9#scrollTo=FDQYGou5b66Y) file that you can copy and run right away, allowing you to quickly experiment with document processing after replacing your API key.\n",
        "\n",
        "**Step 1: Setting Up the Environment**  \n",
        "The first step is ensuring the environment is ready. Installing the Orq SDK is quick and straightforward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaqaxBLgv98I",
        "outputId": "a0d5881d-0d91-4aaa-a23d-d0798692515b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting orq-ai-sdk\n",
            "  Downloading orq_ai_sdk-2.13.4.tar.gz (17 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecation<3.0.0,>=2.1.0 (from orq-ai-sdk)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from orq-ai-sdk)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from orq-ai-sdk) (2.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deprecation<3.0.0,>=2.1.0->orq-ai-sdk) (24.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->orq-ai-sdk) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->orq-ai-sdk) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->orq-ai-sdk) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->orq-ai-sdk) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->orq-ai-sdk) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->orq-ai-sdk) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.1->orq-ai-sdk) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.1->orq-ai-sdk) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.1->orq-ai-sdk) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->orq-ai-sdk) (1.2.2)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: orq-ai-sdk\n",
            "  Building wheel for orq-ai-sdk (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for orq-ai-sdk: filename=orq_ai_sdk-2.13.4-py3-none-any.whl size=23578 sha256=7fc5cb73cc3ace91ace52e3ab9a882f276a700d3e4206c217ed83e3106ab6220\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/a7/7d/823d25efb69295bec7a2355ad01cea27f2c3fb4e3adc9b2d28\n",
            "Successfully built orq-ai-sdk\n",
            "Installing collected packages: deprecation, httpx, orq-ai-sdk\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.0\n",
            "    Uninstalling httpx-0.28.0:\n",
            "      Successfully uninstalled httpx-0.28.0\n",
            "Successfully installed deprecation-2.1.0 httpx-0.27.2 orq-ai-sdk-2.13.4\n"
          ]
        }
      ],
      "source": [
        "!pip install orq-ai-sdk\n",
        "\n",
        "#import\n",
        "import pandas as pd\n",
        "from google.colab import auth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl2U7ACyyf1b"
      },
      "source": [
        "**Step 2: Connecting to Orq**\n",
        "\n",
        "Interacting with Orq’s platform starts with client initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_Dn6BO9yVkI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from orq_ai_sdk import Orq\n",
        "\n",
        "client = Orq(\n",
        "  api_key=os.environ.get(\"ORQ_API_KEY\", \"insert_API_key_here\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkCRI_ea_3nD"
      },
      "source": [
        "**Step 3: Uploading PDF Files for Processing**\n",
        "\n",
        "Here, PDF invoices are uploaded to Orq, making them ready for extraction and analysis. In this case, we have a few PDF files stored in a Google Drive folder that will be used for demonstration. You can easily replace these with your own files to suit your use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJye4_0XWCVw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# API details\n",
        "url = \"https://my.orq.ai/v2/files\"\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer insert_API_key_here\"\n",
        "}\n",
        "\n",
        "# Folder path\n",
        "folder_path = '/content/drive/MyDrive/invoice_test'\n",
        "\n",
        "# Get the first three PDF files\n",
        "pdf_files = [file for file in os.listdir(folder_path) if file.endswith('.pdf')][:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvJV-0uEc7Kj",
        "outputId": "0d26eb85-6e88-48b2-841c-a445b25cffd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing invoice1.pdf: HTTPSConnectionPool(host='my.orq.ai', port=443): Max retries exceeded with url: /v2/files (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2426)')))\n",
            "Failed to upload invoice2.pdf: 403 - {\"message\":\"Forbidden: Invalid token.\"}\n",
            "Error processing invoice3.pdf: HTTPSConnectionPool(host='my.orq.ai', port=443): Max retries exceeded with url: /v2/files (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2426)')))\n"
          ]
        }
      ],
      "source": [
        "# List to store JSON responses\n",
        "responses_json = []\n",
        "\n",
        "# Process each PDF file\n",
        "for file_name in pdf_files:\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "    try:\n",
        "        # Prepare the form data\n",
        "        with open(file_path, 'rb') as file:\n",
        "            files = {\n",
        "                'purpose': (None, 'retrieval'),\n",
        "                'file': (file_name, file)\n",
        "            }\n",
        "\n",
        "            # Send the POST request\n",
        "            response = requests.post(url, headers=headers, files=files)\n",
        "\n",
        "            # Store the JSON response after upload\n",
        "            if response.status_code == 200:\n",
        "                responses_json.append(response.json())\n",
        "                print(f\"Uploaded {file_name}, response stored.\")\n",
        "            else:\n",
        "                print(f\"Failed to upload {file_name}: {response.status_code} - {response.text}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_name}: {e}\")\n",
        "\n",
        "# JSON responses stored in `responses_json` can now be parsed for file_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A02UYTJLBDUB"
      },
      "source": [
        "**Extracting File IDs**  \n",
        "\n",
        "Once the files are uploaded, their unique identifiers (file_ids) are extracted from the responses. These IDs are required for processing in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_LnSPIxb6ff",
        "outputId": "1c3e8a23-d8be-42ac-93fc-5d3cef94e27f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted file_ids: []\n"
          ]
        }
      ],
      "source": [
        "file_ids = [response.get('_id') for response in responses_json if response.get('_id')]\n",
        "print(\"Extracted file_ids:\", file_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pd5jcITBNFB"
      },
      "source": [
        "**Step 4: Deploying for Data Extraction**\n",
        "\n",
        "To ensure consistent and structured outputs from the data extraction process, the GPT-4o model can be configured to adhere to a predefined JSON schema. By specifying the schema, the model is guided to generate results in a precise format, reducing ambiguity and ensuring compatibility with downstream systems.\n",
        "\n",
        "Below is an example schema designed for extracting key fields from receipts, including transaction date, vendor name, and payment details. The schema enforces strict adherence, with required fields and specific data types for each property. This approach ensures that outputs are well-structured and can be directly integrated into applications or databases for further analysis, reporting, or automation. Leveraging this JSON schema with the GPT-4o model enhances the reliability of the extraction process, making it an invaluable tool for handling structured data tasks.\n",
        "\n",
        "This is the prompt in Orq.ai:\n",
        "```plaintext\n",
        "Analyze the provided images of receipts and invoices. Extract the following relevant information:\n",
        "\n",
        "Date: The date of the transaction.\n",
        "Vendor Name: The name of the company or individual from whom the goods or services were purchased.\n",
        "Amount: The total amount spent, including any applicable taxes.\n",
        "Category: An appropriate category for the expense (e.g., Travel, Food, Office Supplies).\n",
        "Payment Method: The method of payment used (e.g., Credit Card, Cash, Bank Transfer).\n",
        "Invoice Number: If available, the unique identifier for the invoice.\n",
        "Map each extracted piece of information to the appropriate columns in a CSV file with the following headers: Date, Vendor Name, Amount, Category, Payment Method, Invoice Number. Provide the results in a structured format suitable for CSV output.\n",
        "\n",
        "This is the receipt:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XEuD_7wB2AS"
      },
      "source": [
        "This is the JSON Schema that helps generate the structured output:\n",
        "```plaintext\n",
        "{\n",
        "  \"name\": \"dataextraction_receipts\",\n",
        "  \"strict\": true,\n",
        "  \"schema\": {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "      \"Date\": {\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"The date of the transaction in YYYY-MM-DD format.\"\n",
        "      },\n",
        "      \"VendorName\": {\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"The name of the company or individual from whom the goods or services were purchased.\"\n",
        "      },\n",
        "      \"Amount\": {\n",
        "        \"type\": \"number\",\n",
        "        \"description\": \"The total amount spent, including any applicable taxes.\"\n",
        "      },\n",
        "      \"Category\": {\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"An appropriate category for the expense (e.g., Travel, Food, Office Supplies).\"\n",
        "      },\n",
        "      \"PaymentMethod\": {\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"The method of payment used (e.g., Credit Card, Cash, Bank Transfer).\"\n",
        "      },\n",
        "      \"InvoiceNumber\": {\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"The unique identifier for the invoice, if available.\"\n",
        "      }\n",
        "    },\n",
        "    \"additionalProperties\": false,\n",
        "    \"required\": [\n",
        "      \"Date\",\n",
        "      \"VendorName\",\n",
        "      \"Amount\",\n",
        "      \"Category\",\n",
        "      \"PaymentMethod\",\n",
        "      \"InvoiceNumber\"\n",
        "    ]\n",
        "  }\n",
        "}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3yhBo6QCC3P"
      },
      "source": [
        "With file_ids in hand, the next step is invoking a pre-trained deployment to extract structured data from the invoices. This process transforms raw document data into insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDQYGou5b66Y"
      },
      "outputs": [],
      "source": [
        "# Iterate through each file_id and invoke the deployment\n",
        "for file_id in file_ids:\n",
        "    try:\n",
        "        generation = client.deployments.invoke(\n",
        "            key=\"DataExtraction_Receipts\",\n",
        "            context={\n",
        "                \"environments\": []\n",
        "            },\n",
        "            file_ids=[file_id],  # Use a single file_id in the list\n",
        "            metadata={\n",
        "                \"custom-field-name\": \"custom-metadata-value\"\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Print the content for each invocation\n",
        "        print(f\"Response for file_id {file_id}: {generation.choices[0].message.content}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error invoking deployment for file_id {file_id}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUqMTCa8CENY"
      },
      "source": [
        "**What’s Next?**  \n",
        "Orq’s tools provide robust capabilities for extracting structured data from unstructured PDF documents. With this workflow, you can:\n",
        "\n",
        "- Scale Data Processing: Adapt the workflow to handle larger batches of PDF files or seamlessly integrate it into your existing systems.\n",
        "- Refine Extraction Outputs: Leverage Orq’s deployment configurations to fine-tune the extraction process for specific document formats, layouts, or fields.\n",
        "- Automate End-to-End Workflows: Combine this process with automated pipelines to optimize tasks such as invoice management, financial reporting, or compliance monitoring.\n",
        "\n",
        "By transforming unstructured PDF data into actionable insights, Orq empowers businesses to streamline operations, improve decision-making, and unlock new efficiencies with ease."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
